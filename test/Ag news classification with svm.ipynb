{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4551c8",
   "metadata": {},
   "source": [
    "# üìÇ Data Discovery\n",
    "\n",
    "In this section, we will perform an initial inspection of the dataset files to understand their structure and content before loading them into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "train_path = r\"C:\\Users\\yassi\\Documents\\projects\\MlOps Ag news classification\\dataset\\train.csv\"\n",
    "test_path = r\"C:\\Users\\yassi\\Documents\\projects\\MlOps Ag news classification\\dataset\\test.csv\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç FILE DISCOVERY - Training Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Read first few lines without processing\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    print(\"\\nFirst 5 raw lines:\")\n",
    "    for i in range(5):\n",
    "        line = f.readline().strip()\n",
    "        print(f\"Line {i+1}: {line[:150]}...\")  # Show first 150 chars\n",
    "\n",
    "# Load without assumptions\n",
    "train_sample = pd.read_csv(train_path, header=None, nrows=10)\n",
    "print(f\"\\nüìä Shape: {train_sample.shape}\")\n",
    "print(f\"üìä Number of columns detected: {train_sample.shape[1]}\")\n",
    "print(f\"\\nüìã Column data types:\\n{train_sample.dtypes}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"First 10 rows:\")\n",
    "print(\"=\"*70)\n",
    "print(train_sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç FILE DISCOVERY - Test Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check test file too\n",
    "test_sample = pd.read_csv(test_path, header=None, nrows=10)\n",
    "print(f\"\\nüìä Shape: {test_sample.shape}\")\n",
    "print(f\"üìä Number of columns detected: {test_sample.shape[1]}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(test_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9177b13",
   "metadata": {},
   "source": [
    "# üìä Exploratory Data Analysis (EDA)\n",
    "\n",
    "We will now perform a comprehensive Exploratory Data Analysis to understand the dataset characteristics, class distribution, and text properties. This step is crucial for informing our preprocessing and modeling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74db8ff1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "We start by importing the necessary libraries for data manipulation, visualization, and analysis. Then, we load the training and testing datasets and display basic information about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load data WITH headers\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nTotal samples: {len(train_df) + len(test_df):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã Column Names:\")\n",
    "print(\"=\"*70)\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã Data Types:\")\n",
    "print(\"=\"*70)\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã First 5 Training Examples:\")\n",
    "print(\"=\"*70)\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã Dataset Info:\")\n",
    "print(\"=\"*70)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72857539",
   "metadata": {},
   "source": [
    "## 2. Label Distribution Analysis\n",
    "\n",
    "Understanding the distribution of target labels is essential to identify any class imbalance issues. We will calculate and display the counts and percentages for each category in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81150709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AG News label mapping (0-indexed!)\n",
    "label_mapping = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\", \n",
    "    2: \"Business\",\n",
    "    3: \"Science/Tech\"\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä LABEL DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training set labels\n",
    "print(\"\\nüîπ TRAINING SET:\")\n",
    "print(\"-\" * 70)\n",
    "train_label_counts = train_df['label'].value_counts().sort_index()\n",
    "print(train_label_counts)\n",
    "print(f\"\\nTotal: {train_label_counts.sum():,}\")\n",
    "\n",
    "# Add percentages\n",
    "print(\"\\nWith percentages:\")\n",
    "for label, count in train_label_counts.items():\n",
    "    pct = (count / len(train_df)) * 100\n",
    "    category = label_mapping[label]\n",
    "    print(f\"Label {label} ({category:15s}): {count:6,} samples ({pct:.2f}%)\")\n",
    "\n",
    "# Test set labels\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîπ TEST SET:\")\n",
    "print(\"-\" * 70)\n",
    "test_label_counts = test_df['label'].value_counts().sort_index()\n",
    "print(test_label_counts)\n",
    "print(f\"\\nTotal: {test_label_counts.sum():,}\")\n",
    "\n",
    "# Add percentages\n",
    "print(\"\\nWith percentages:\")\n",
    "for label, count in test_label_counts.items():\n",
    "    pct = (count / len(test_df)) * 100\n",
    "    category = label_mapping[label]\n",
    "    print(f\"Label {label} ({category:15s}): {count:6,} samples ({pct:.2f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚öñÔ∏è CLASS BALANCE CHECK\")\n",
    "print(\"=\"*70)\n",
    "train_balance = train_label_counts.max() / train_label_counts.min()\n",
    "test_balance = test_label_counts.max() / test_label_counts.min()\n",
    "print(f\"Training set imbalance ratio: {train_balance:.2f}\")\n",
    "print(f\"Test set imbalance ratio: {test_balance:.2f}\")\n",
    "\n",
    "if train_balance == 1.0:\n",
    "    print(\"‚úÖ Classes are PERFECTLY BALANCED!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Classes are imbalanced (ratio: {train_balance:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bec660",
   "metadata": {},
   "source": [
    "## 3. Visualization of Label Distribution\n",
    "\n",
    "Visualizing the label distribution helps in quickly assessing the balance of the dataset. We will use bar charts to show the number of samples per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbfeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_palette(\"husl\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "categories = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "train_counts = [train_label_counts[i] for i in sorted(train_label_counts.index)]\n",
    "\n",
    "axes[0].bar(categories, train_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
    "axes[0].set_title('Training Set Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Category', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[0].set_ylim(0, 35000)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(train_counts):\n",
    "    axes[0].text(i, v + 500, f'{v:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Test set\n",
    "test_counts = [test_label_counts[i] for i in sorted(test_label_counts.index)]\n",
    "\n",
    "axes[1].bar(categories, test_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
    "axes[1].set_title('Test Set Label Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Category', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[1].set_ylim(0, 2200)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(test_counts):\n",
    "    axes[1].text(i, v + 50, f'{v:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2ef83",
   "metadata": {},
   "source": [
    "## 4. Text Length Analysis\n",
    "\n",
    "Analyzing the length of the text samples (in terms of characters and words) can provide insights into the data quality and help in setting parameters for models (e.g., max sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length columns\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "test_df['word_count'] = test_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìù TEXT LENGTH STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîπ TRAINING SET:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nCharacter Length:\")\n",
    "print(train_df['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count:\")\n",
    "print(train_df['word_count'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîπ TEST SET:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nCharacter Length:\")\n",
    "print(test_df['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count:\")\n",
    "print(test_df['word_count'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TEXT LENGTH BY CATEGORY (Training Set)\")\n",
    "print(\"=\"*70)\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    category = label_mapping[label]\n",
    "    subset = train_df[train_df['label'] == label]\n",
    "    avg_length = subset['text_length'].mean()\n",
    "    avg_words = subset['word_count'].mean()\n",
    "    print(f\"\\n{category:15s}: Avg chars: {avg_length:.0f} | Avg words: {avg_words:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d24bf5",
   "metadata": {},
   "source": [
    "## 5. Visualization of Text Length Distributions\n",
    "\n",
    "We visualize the distribution of text lengths and word counts using histograms and box plots. This helps in identifying outliers and understanding the variability across different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Text length distribution (training)\n",
    "axes[0, 0].hist(train_df['text_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axvline(train_df['text_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_df[\"text_length\"].mean():.0f}')\n",
    "axes[0, 0].axvline(train_df['text_length'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {train_df[\"text_length\"].median():.0f}')\n",
    "axes[0, 0].set_title('Training Set - Character Length Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Characters', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Word count distribution (training)\n",
    "axes[0, 1].hist(train_df['word_count'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axvline(train_df['word_count'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {train_df[\"word_count\"].mean():.1f}')\n",
    "axes[0, 1].axvline(train_df['word_count'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {train_df[\"word_count\"].median():.0f}')\n",
    "axes[0, 1].set_title('Training Set - Word Count Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Words', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Text length by category\n",
    "category_data = []\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    category_data.append(train_df[train_df['label'] == label]['text_length'])\n",
    "\n",
    "axes[1, 0].boxplot(category_data, labels=categories, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 0].set_title('Text Length by Category (Training)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Category', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Character Length', fontsize=10)\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Word count by category\n",
    "word_data = []\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    word_data.append(train_df[train_df['label'] == label]['word_count'])\n",
    "\n",
    "axes[1, 1].boxplot(word_data, labels=categories, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "axes[1, 1].set_title('Word Count by Category (Training)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Category', fontsize=10)\n",
    "axes[1, 1].set_ylabel('Word Count', fontsize=10)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Text length visualizations created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16b33f",
   "metadata": {},
   "source": [
    "## 6. Sample Texts Inspection\n",
    "\n",
    "It is always good practice to manually inspect some random samples from each category to get a feel for the data and verify that the labels make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìñ SAMPLE TEXTS FROM EACH CATEGORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show 2 examples from each category\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    category = label_mapping[label]\n",
    "    samples = train_df[train_df['label'] == label].sample(2, random_state=42)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üè∑Ô∏è  {category.upper()} (Label: {label})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for idx, (i, row) in enumerate(samples.iterrows(), 1):\n",
    "        text = row['text']\n",
    "        text_preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "        print(f\"\\nExample {idx}:\")\n",
    "        print(f\"Length: {len(text)} chars | {len(text.split())} words\")\n",
    "        print(f\"Text: {text_preview}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ Sample texts displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413019c",
   "metadata": {},
   "source": [
    "## 7. Most Common Words Analysis\n",
    "\n",
    "We analyze the most frequent words in each category (excluding stopwords) to identify keywords that might be predictive of the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_top_words(texts, n=15):\n",
    "    \"\"\"Get top N most common words (excluding stopwords)\"\"\"\n",
    "    # Simple stopwords\n",
    "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',\n",
    "                 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',\n",
    "                 'would', 'should', 'could', 'may', 'might', 'must', 'can', 'this',\n",
    "                 'that', 'these', 'those', 'it', 'its', 'their', 'them', 'they', 'he',\n",
    "                 'she', 'his', 'her', 'him', 'i', 'you', 'we', 'us', 'our', 'your'}\n",
    "    \n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        # Convert to lowercase and extract words\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "        # Filter stopwords and short words\n",
    "        words = [w for w in words if w not in stopwords and len(w) > 2]\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    return Counter(all_words).most_common(n)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî§ TOP 15 WORDS BY CATEGORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for label in sorted(train_df['label'].unique()):\n",
    "    category = label_mapping[label]\n",
    "    texts = train_df[train_df['label'] == label]['text']\n",
    "    top_words = get_top_words(texts, n=15)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìå {category.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for i, (word, count) in enumerate(top_words, 1):\n",
    "        print(f\"{i:2d}. {word:15s} : {count:6,} occurrences\")\n",
    "\n",
    "print(\"\\n‚úÖ Top words analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa685e",
   "metadata": {},
   "source": [
    "## 8. Word Cloud Generation\n",
    "\n",
    "Word clouds provide a visual representation of the most frequent words in each category, allowing for a quick intuitive understanding of the main topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Color schemes for each category\n",
    "colors = ['Reds', 'Blues', 'Greens', 'Purples']\n",
    "\n",
    "for idx, label in enumerate(sorted(train_df['label'].unique())):\n",
    "    category = label_mapping[label]\n",
    "    \n",
    "    # Get all text for this category\n",
    "    category_text = ' '.join(train_df[train_df['label'] == label]['text'].values)\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        colormap=colors[idx],\n",
    "        max_words=100,\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(category_text)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[idx].set_title(f'{category}', fontsize=16, fontweight='bold', pad=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Word clouds created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbede724",
   "metadata": {},
   "source": [
    "## 9. EDA Summary and Insights\n",
    "\n",
    "We summarize the key findings from our Exploratory Data Analysis, highlighting the dataset characteristics, data quality, and implications for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä EXPLORATORY DATA ANALYSIS - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ DATASET CHARACTERISTICS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Training samples:     {len(train_df):,}\")\n",
    "print(f\"Test samples:         {len(test_df):,}\")\n",
    "print(f\"Total samples:        {len(train_df) + len(test_df):,}\")\n",
    "print(f\"Number of classes:    {train_df['label'].nunique()}\")\n",
    "print(f\"Features:             {list(train_df.columns)}\")\n",
    "\n",
    "print(\"\\n‚úÖ CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Perfect balance: Each class has exactly 25% of data\")\n",
    "print(\"‚Ä¢ World:        30,000 train | 1,900 test\")\n",
    "print(\"‚Ä¢ Sports:       30,000 train | 1,900 test\")\n",
    "print(\"‚Ä¢ Business:     30,000 train | 1,900 test\")\n",
    "print(\"‚Ä¢ Science/Tech: 30,000 train | 1,900 test\")\n",
    "\n",
    "print(\"\\n‚úÖ TEXT CHARACTERISTICS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Average text length:  {train_df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Average word count:   {train_df['word_count'].mean():.1f} words\")\n",
    "print(f\"Median text length:   {train_df['text_length'].median():.0f} characters\")\n",
    "print(f\"Median word count:    {train_df['word_count'].median():.0f} words\")\n",
    "print(f\"Min text length:      {train_df['text_length'].min()} characters\")\n",
    "print(f\"Max text length:      {train_df['text_length'].max()} characters\")\n",
    "\n",
    "print(\"\\n‚úÖ DATA QUALITY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Missing values (train): {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Missing values (test):  {test_df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows (train): {train_df.duplicated().sum()}\")\n",
    "print(f\"Duplicate rows (test):  {test_df.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n‚úÖ CATEGORY DISTINCTIVENESS\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ World:        Political/conflict terms (iraq, president, minister)\")\n",
    "print(\"‚Ä¢ Sports:       Sports-specific terms (game, team, season, cup)\")\n",
    "print(\"‚Ä¢ Business:     Financial terms (stocks, oil, prices, profit)\")\n",
    "print(\"‚Ä¢ Science/Tech: Technology terms (microsoft, software, internet)\")\n",
    "\n",
    "print(\"\\n‚úÖ KEY INSIGHTS FOR MODELING\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. ‚úÖ Perfectly balanced classes ‚Üí No need for resampling\")\n",
    "print(\"2. ‚úÖ Clear vocabulary separation ‚Üí Good for classification\")\n",
    "print(\"3. ‚úÖ Consistent text lengths ‚Üí Stable model input\")\n",
    "print(\"4. ‚úÖ No missing data ‚Üí Clean dataset\")\n",
    "print(\"5. ‚úÖ Large dataset (120k) ‚Üí Good for deep learning\")\n",
    "print(\"6. ‚úÖ Reasonable text length (avg 38 words) ‚Üí Fits transformers\")\n",
    "\n",
    "print(\"\\n‚úÖ RECOMMENDED MODEL APPROACH\")\n",
    "print(\"-\" * 70)\n",
    "print(\"‚Ä¢ Model: DistilBERT (efficient transformer)\")\n",
    "print(\"‚Ä¢ Tokenizer: distilbert-base-uncased\")\n",
    "print(\"‚Ä¢ Max sequence length: 128 tokens (covers 95% of texts)\")\n",
    "print(\"‚Ä¢ Batch size: 32 (good balance)\")\n",
    "print(\"‚Ä¢ Expected accuracy: 94%+ (based on literature)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ EDA COMPLETE - READY FOR MODEL TRAINING!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046080e4",
   "metadata": {},
   "source": [
    "# üßπ Data Preprocessing\n",
    "\n",
    "Data preprocessing is a critical step in NLP. We will clean and normalize the text data to prepare it for vectorization and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dac0d",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Pipeline Definition\n",
    "\n",
    "We define a comprehensive preprocessing function that includes lowercasing, removing HTML tags, URLs, numbers, punctuation, and stopwords, as well as lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model (you already have it)\n",
    "print(\"‚è≥ Loading spaCy en_core_web_sm...\")\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "print(\"‚úÖ Loaded!\")\n",
    "\n",
    "# Get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline:\n",
    "    1. Lowercase\n",
    "    2. Fix HTML entities (#39; ‚Üí ')\n",
    "    3. Remove emails\n",
    "    4. Remove HTML tags\n",
    "    5. Remove URLs\n",
    "    6. Remove numbers\n",
    "    7. Expand contractions\n",
    "    8. Remove punctuation\n",
    "    9. Remove extra whitespace\n",
    "    10. Remove stopwords\n",
    "    11. Lemmatization\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Fix HTML entities like #39; ‚Üí '\n",
    "    text = re.sub(r'#39;', \"'\", text)\n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    text = re.sub(r'&quot;', '\"', text)\n",
    "    \n",
    "    # 3. Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 5. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 6. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 7. Expand contractions (can't ‚Üí cannot, won't ‚Üí will not)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # 8. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 9. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 10 & 11. Remove stopwords + Lemmatization (using spaCy)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and len(token.text) > 2]\n",
    "    \n",
    "    # Join back into text\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Preprocessing function ready!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on a sample\n",
    "test_text = \"Wall St. Bears 32#8 & Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\"\n",
    "\n",
    "print(\"\\nüìù BEFORE:\")\n",
    "print(test_text)\n",
    "\n",
    "print(\"\\nüìù AFTER:\")\n",
    "processed = preprocess_text(test_text)\n",
    "print(processed)\n",
    "\n",
    "print(\"\\n‚úÖ Test complete! Ready to process all data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75f721",
   "metadata": {},
   "source": [
    "## 2. Applying Preprocessing\n",
    "\n",
    "We apply the defined preprocessing pipeline to the entire training and test datasets. This may take some time due to the size of the dataset and the complexity of the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d71c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enable progress bar for pandas\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß APPLYING PREPROCESSING TO FULL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply to training data\n",
    "print(\"\\n‚è≥ Processing training data (120,000 samples)...\")\n",
    "print(\"   This may take 5-10 minutes...\")\n",
    "train_df['text_clean'] = train_df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Apply to test data\n",
    "print(\"\\n‚è≥ Processing test data (7,600 samples)...\")\n",
    "test_df['text_clean'] = test_df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù PREPROCESSING RESULTS - EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"BEFORE: {train_df['text'].iloc[i][:150]}...\")\n",
    "    print(f\"AFTER:  {train_df['text_clean'].iloc[i][:150]}...\")\n",
    "\n",
    "# Check text length after preprocessing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TEXT LENGTH AFTER PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "train_df['clean_word_count'] = train_df['text_clean'].str.split().str.len()\n",
    "print(f\"Average words (before): {train_df['word_count'].mean():.1f}\")\n",
    "print(f\"Average words (after):  {train_df['clean_word_count'].mean():.1f}\")\n",
    "print(f\"Reduction: {(1 - train_df['clean_word_count'].mean()/train_df['word_count'].mean())*100:.1f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Data ready for vectorization!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b4d6b",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Parameter Analysis\n",
    "\n",
    "Before vectorizing, we analyze the preprocessed text to determine optimal parameters for TF-IDF, such as vocabulary size and document frequency thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ANALYZING PREPROCESSED DATA FOR TF-IDF PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all preprocessed text for analysis\n",
    "all_texts = train_df['text_clean'].tolist()\n",
    "\n",
    "# 1. VOCABULARY ANALYSIS\n",
    "print(\"\\nüî§ VOCABULARY ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get all words\n",
    "all_words = ' '.join(all_texts).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"Total words (tokens):     {len(all_words):,}\")\n",
    "print(f\"Unique words (vocab):     {len(word_freq):,}\")\n",
    "print(f\"Average word frequency:   {len(all_words)/len(word_freq):.2f}\")\n",
    "\n",
    "# 2. WORD FREQUENCY DISTRIBUTION\n",
    "print(\"\\nüìä WORD FREQUENCY DISTRIBUTION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Count words by frequency ranges\n",
    "freq_1 = sum(1 for w, c in word_freq.items() if c == 1)\n",
    "freq_2_5 = sum(1 for w, c in word_freq.items() if 2 <= c <= 5)\n",
    "freq_6_10 = sum(1 for w, c in word_freq.items() if 6 <= c <= 10)\n",
    "freq_11_50 = sum(1 for w, c in word_freq.items() if 11 <= c <= 50)\n",
    "freq_51_100 = sum(1 for w, c in word_freq.items() if 51 <= c <= 100)\n",
    "freq_100_plus = sum(1 for w, c in word_freq.items() if c > 100)\n",
    "\n",
    "print(f\"Words appearing 1 time:       {freq_1:,} ({freq_1/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words appearing 2-5 times:    {freq_2_5:,} ({freq_2_5/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words appearing 6-10 times:   {freq_6_10:,} ({freq_6_10/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words appearing 11-50 times:  {freq_11_50:,} ({freq_11_50/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words appearing 51-100 times: {freq_51_100:,} ({freq_51_100/len(word_freq)*100:.1f}%)\")\n",
    "print(f\"Words appearing 100+ times:   {freq_100_plus:,} ({freq_100_plus/len(word_freq)*100:.1f}%)\")\n",
    "\n",
    "# 3. TOP WORDS (most common)\n",
    "print(\"\\nüìà TOP 20 MOST COMMON WORDS\")\n",
    "print(\"-\" * 70)\n",
    "for word, count in word_freq.most_common(20):\n",
    "    print(f\"{word:20s}: {count:,}\")\n",
    "\n",
    "# 4. DOCUMENT LENGTH ANALYSIS\n",
    "print(\"\\nüìè DOCUMENT LENGTH (after preprocessing)\")\n",
    "print(\"-\" * 70)\n",
    "doc_lengths = [len(text.split()) for text in all_texts]\n",
    "print(f\"Min words:    {min(doc_lengths)}\")\n",
    "print(f\"Max words:    {max(doc_lengths)}\")\n",
    "print(f\"Mean words:   {np.mean(doc_lengths):.1f}\")\n",
    "print(f\"Median words: {np.median(doc_lengths):.1f}\")\n",
    "print(f\"Std words:    {np.std(doc_lengths):.1f}\")\n",
    "\n",
    "# 5. DOCUMENT FREQUENCY ANALYSIS\n",
    "print(\"\\nüìÑ DOCUMENT FREQUENCY ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Use CountVectorizer to get document frequencies\n",
    "count_vec = CountVectorizer()\n",
    "count_matrix = count_vec.fit_transform(all_texts)\n",
    "doc_freq = (count_matrix > 0).sum(axis=0).A1  # Document frequency for each word\n",
    "\n",
    "vocab_list = count_vec.get_feature_names_out()\n",
    "df_dict = dict(zip(vocab_list, doc_freq))\n",
    "\n",
    "# Analyze document frequency distribution\n",
    "total_docs = len(all_texts)\n",
    "df_below_3 = sum(1 for df in doc_freq if df < 3)\n",
    "df_above_95pct = sum(1 for df in doc_freq if df > 0.95 * total_docs)\n",
    "\n",
    "print(f\"Total documents: {total_docs:,}\")\n",
    "print(f\"Words in < 3 docs (rare):      {df_below_3:,} ({df_below_3/len(vocab_list)*100:.1f}%)\")\n",
    "print(f\"Words in > 95% docs (common):  {df_above_95pct:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb01c65",
   "metadata": {},
   "source": [
    "## 4. Optimal TF-IDF Parameters Determination\n",
    "\n",
    "Based on the analysis, we select the best parameters for the TF-IDF vectorizer to balance feature richness and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä DETERMINING OPTIMAL TF-IDF PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate how many words remain after filtering\n",
    "print(\"\\nüîç IMPACT OF min_df PARAMETER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for min_df_val in [1, 2, 3, 5, 10]:\n",
    "    remaining = sum(1 for df in doc_freq if df >= min_df_val)\n",
    "    pct = remaining / len(vocab_list) * 100\n",
    "    print(f\"min_df={min_df_val:2d}: {remaining:,} words remain ({pct:.1f}%)\")\n",
    "\n",
    "# Calculate ngram impact\n",
    "print(\"\\nüîç NGRAM ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample for speed\n",
    "sample_texts = train_df['text_clean'].sample(10000, random_state=42).tolist()\n",
    "\n",
    "# Unigrams only\n",
    "vec_1gram = CountVectorizer(ngram_range=(1,1), min_df=3)\n",
    "vec_1gram.fit(sample_texts)\n",
    "print(f\"Unigrams (1,1) with min_df=3:           {len(vec_1gram.get_feature_names_out()):,} features\")\n",
    "\n",
    "# Unigrams + Bigrams\n",
    "vec_2gram = CountVectorizer(ngram_range=(1,2), min_df=3)\n",
    "vec_2gram.fit(sample_texts)\n",
    "print(f\"Unigrams + Bigrams (1,2) with min_df=3: {len(vec_2gram.get_feature_names_out()):,} features\")\n",
    "\n",
    "# Unigrams + Bigrams + Trigrams\n",
    "vec_3gram = CountVectorizer(ngram_range=(1,3), min_df=3)\n",
    "vec_3gram.fit(sample_texts)\n",
    "print(f\"Uni + Bi + Trigrams (1,3) with min_df=3: {len(vec_3gram.get_feature_names_out()):,} features\")\n",
    "\n",
    "# Show sample bigrams that might be useful\n",
    "print(\"\\nüìã SAMPLE BIGRAMS (might capture context):\")\n",
    "print(\"-\" * 70)\n",
    "bigram_features = [f for f in vec_2gram.get_feature_names_out() if ' ' in f]\n",
    "print(bigram_features[:30])\n",
    "\n",
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ RECOMMENDED TF-IDF PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommended_params = {\n",
    "    'max_features': 15000,       # Capture most important features\n",
    "    'min_df': 3,                 # Remove words appearing in < 3 docs (removes 62.7% noise)\n",
    "    'max_df': 0.95,              # Remove words in > 95% docs\n",
    "    'ngram_range': (1, 2),       # Unigrams + Bigrams (captures phrases)\n",
    "    'sublinear_tf': True,        # Log scaling (reduces impact of high freq words)\n",
    "}\n",
    "\n",
    "print(\"\\nBased on data analysis:\")\n",
    "for param, value in recommended_params.items():\n",
    "    print(f\"  {param:20s}: {value}\")\n",
    "\n",
    "print(\"\\nüìù RATIONALE:\")\n",
    "print(\"  ‚Ä¢ min_df=3: Removes 62.7% rare/noisy words\")\n",
    "print(\"  ‚Ä¢ max_features=15000: Keeps top features, manageable size\")\n",
    "print(\"  ‚Ä¢ ngram_range=(1,2): Bigrams capture context like 'oil price', 'world cup'\")\n",
    "print(\"  ‚Ä¢ sublinear_tf=True: Log scaling helps with word like 'reuters' (28k occurrences)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464784d9",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Vectorization and Data Splitting\n",
    "\n",
    "We split the training data into training and validation sets, and then apply the TF-IDF vectorization using the optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä APPLYING TF-IDF VECTORIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Create train/validation split\n",
    "print(\"\\n‚è≥ Creating train/validation split...\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df['text_clean'], \n",
    "    train_df['label'],\n",
    "    test_size=0.1,  # 10% for validation\n",
    "    random_state=42,\n",
    "    stratify=train_df['label']  # Keep class balance\n",
    ")\n",
    "\n",
    "X_test = test_df['text_clean']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(\"\\nüìä DATA SPLITS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Training set:   {len(X_train):,} samples\")\n",
    "print(f\"Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Test set:       {len(X_test):,} samples\")\n",
    "\n",
    "# 2. Apply TF-IDF with optimized parameters\n",
    "print(\"\\n‚è≥ Applying TF-IDF Vectorization with optimized parameters...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=15000,      # Top 15,000 features\n",
    "    min_df=3,                # Remove words in < 3 docs\n",
    "    max_df=0.95,             # Remove words in > 95% docs  \n",
    "    ngram_range=(1, 2),      # Unigrams + Bigrams\n",
    "    sublinear_tf=True        # Log scaling\n",
    ")\n",
    "\n",
    "# Fit on training data only, transform all\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ TF-IDF Vectorization complete!\")\n",
    "\n",
    "print(\"\\nüìä FEATURE MATRIX SHAPES:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Train: {X_train_tfidf.shape} ({X_train_tfidf.shape[0]:,} samples √ó {X_train_tfidf.shape[1]:,} features)\")\n",
    "print(f\"Val:   {X_val_tfidf.shape} ({X_val_tfidf.shape[0]:,} samples √ó {X_val_tfidf.shape[1]:,} features)\")\n",
    "print(f\"Test:  {X_test_tfidf.shape} ({X_test_tfidf.shape[0]:,} samples √ó {X_test_tfidf.shape[1]:,} features)\")\n",
    "\n",
    "# 3. Verify class balance\n",
    "print(\"\\n‚öñÔ∏è CLASS DISTRIBUTION IN SPLITS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Train:      {dict(sorted(Counter(y_train).items()))}\")\n",
    "print(f\"Validation: {dict(sorted(Counter(y_val).items()))}\")\n",
    "print(f\"Test:       {dict(sorted(Counter(y_test).items()))}\")\n",
    "\n",
    "# 4. Show top features\n",
    "print(\"\\nüìã TOP 30 TF-IDF FEATURES:\")\n",
    "print(\"-\" * 70)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(list(feature_names[:30]))\n",
    "\n",
    "# Save for later use\n",
    "print(\"\\n‚úÖ Data ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691586f3",
   "metadata": {},
   "source": [
    "# ü§ñ Model Training\n",
    "\n",
    "In this section, we will train and evaluate several baseline machine learning models to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f9059",
   "metadata": {},
   "source": [
    "## 1. Baseline Model Training\n",
    "\n",
    "We train multiple models including Logistic Regression, Naive Bayes, Linear SVM, and Random Forest, and compare their performance on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c59f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ TRAINING BASELINE MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define models to compare (n_jobs=4 for parallelization)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, n_jobs=4),\n",
    "    'Naive Bayes (Multinomial)': MultinomialNB(),  # No n_jobs param\n",
    "    'Linear SVM': LinearSVC(max_iter=2000, random_state=42),  # No n_jobs param\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=4)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìå Training: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on validation\n",
    "    y_val_pred = model.predict(X_val_tfidf)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Predict on test\n",
    "    y_test_pred = model.predict(X_test_tfidf)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_time': train_time,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  Training time:     {train_time:.2f} seconds\")\n",
    "    print(f\"üìä Validation accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "    print(f\"üìä Test accuracy:       {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BASELINE MODELS COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<30} {'Val Acc':>10} {'Test Acc':>10} {'Time (s)':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, res in sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True):\n",
    "    print(f\"{name:<30} {res['val_accuracy']*100:>9.2f}% {res['test_accuracy']*100:>9.2f}% {res['train_time']:>10.2f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['test_accuracy'])\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name} ({results[best_model_name]['test_accuracy']*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f5849",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation\n",
    "\n",
    "We perform a detailed evaluation of the best-performing model (Linear SVM), including classification report, confusion matrix, and misclassification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e60169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä DETAILED EVALUATION: LINEAR SVM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get predictions from best model\n",
    "best_model = results['Linear SVM']['model']\n",
    "y_test_pred = results['Linear SVM']['y_test_pred']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=list(label_mapping.values())))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nüìä CONFUSION MATRIX:\")\n",
    "print(\"-\" * 70)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìä PER-CLASS ACCURACY:\")\n",
    "print(\"-\" * 70)\n",
    "for i, category in label_mapping.items():\n",
    "    class_acc = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"{category:15s}: {class_acc*100:.2f}% ({cm[i,i]}/{cm[i,:].sum()})\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=list(label_mapping.values()),\n",
    "            yticklabels=list(label_mapping.values()),\n",
    "            annot_kws={'size': 14})\n",
    "plt.title('Confusion Matrix - Linear SVM', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Misclassification analysis\n",
    "print(\"\\nüîç MISCLASSIFICATION ANALYSIS:\")\n",
    "print(\"-\" * 70)\n",
    "total_errors = (y_test != y_test_pred).sum()\n",
    "print(f\"Total errors: {total_errors} out of {len(y_test)} ({total_errors/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nMost common confusions:\")\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if i != j and cm[i,j] > 50:  # Show confusions > 50\n",
    "            print(f\"  {label_mapping[i]:12s} ‚Üí {label_mapping[j]:12s}: {cm[i,j]} errors\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
